## 多粒度结构知识蒸馏进行语言模型压缩

#### 摘要

近年来，将知识通过蒸馏的方式从大模型转移到小模型获得越来越多的关注。以前的方式主要从单粒度语言单元中获取知识并进行知识转移，这种方式不能很好的表示文本中丰富的语义信息，可能回丢失某些重要信息——忽视了模型中的结构信息。

本论文提出了一个新的知识蒸馏框架

1. 从多粒度语义中提取中间表示（tokens、spans和samples）
2. 将知识形成为更复杂的结构关系，指定为基于多粒度表示的二元交互pair-wise interactions和三元几何角度triplet-wise geometric angles
3. 将组织良好的多粒度结构知识按层蒸馏给学生模型

在GLUE基准测试上实现了SOTA

#### 1 介绍

预训练语言模型基于Transformer架构和使用自监督目标在大规模语料库进行预训练，在自然语言理解和生成任务上取得了很大的成功。但这些PLMs往往参数量巨大，需要极高的计算量和内存占用，不适用于资源受限的设备，如手机和嵌入式设备。

模型压缩主要就是在最小性能损失的前提下，将大型的更复杂模型压缩成参数量小、计算量更小的模型；知识蒸馏就是目前比较主流的模型压缩方法。

现有的知识蒸馏方法：基于师生模型、提炼的知识可以来着输出信息和中间表示，从教师模型中提炼token级表示和attention依赖并传递给学生模型

**问题**：

1. 只能适应单粒度语言单元的表示（token级和文本级），而忽略了其他粒度
2. 蒸馏目标是对齐教师和学生的相应表示或者注意力依赖关系，无法捕获表示之间更复杂的结构关系

多粒度结构知识蒸馏——MGSKD

1. which——知识应该属于哪个粒度
   * 三个粒度：token、span、sample
     * **tokens**是最小的语义粒度，通常是单个单词或标点符号。在自然语言处理任务中，我们通常会将文本拆分为tokens，然后对其进行分析或建模。
     * **spans**是一组tokens，通常是一个短语或句子。在自然语言处理任务中，我们可能会使用spans来捕捉更高层次的语义信息，例如实体识别或句法分析。
     * **samples**是一组spans或整个文本，通常是一个文档或数据集。在自然语言处理任务中，我们可能会使用samples来捕捉更高层次的语义信息，例如主题建模或情感分析。
   * 使用**平均池化**来获得基于token表示的span表示和sample表示
2. what——知识被表示成什么形式能够更高效地转移
   * 不是直接将教师模型和学生模型的相应表示进行对齐
   * 把表示的pair-wise interactions和triplet-wise geometric angles等结构关系信息作为知识
3. how——如何将知识转移到学生模型
   * 底层捕获语法特征，上层编码语义特征
   * 分层蒸馏——学生底层学习token级和span级特征，上层学习sample级特征

在基准测试集GLUE上进行实验，发现论文中提出的框架优于strong baseline方法，甚至在GLUE上的部分任务中表现要好于$BERT_{base}$，同时模型更小更快

**贡献**：

1. 首次将语言中的多粒度语义表示应用于知识蒸馏
2. 将多粒度表示的pair-wise interactions和triplet-wise geometric angles作为知识，该知识整合了不同表示复杂的结构信息
3. 在GLUE基准测试集上进行了综合实验， MGSKD比其他知识蒸馏baseline取得了更好的结果

#### 2. 相关工作

**语言模型压缩**

预训练语言模型在很多任务上表现很好，但是有很大的计算和内存消耗；为了将这些大模型部署到资源受限的设备上，尝试了各种方法将语言模型压缩成小型模型；量化——将模型参数转换为低精度表示；剪枝——识别不重要的单个权重和结构；权重共享技术——允许模型多次重用transformer层以减少参数

**知识蒸馏**

模型压缩的一个重要研究方向，早期知识蒸馏方法：

1. Hinton首次提出最小化学生和教师模型预测分布的KL散度，之后被应用到掩码语言建模和文本分类任务上
2. Romero提出直接匹配教师和学生的特征激活值，Jiao在此基础上将教师模型transformer层的中间表示作为知识进行传递
3. Tian提出了对比框架，教师的表示是相应学生表示的一个positive
4. 在图像分类领域，大量文献表明教师模型中的图片表示的关系信息应该被保留在学生模型中，并采用一系列几何测量对样本关系进行建模。
5. 对于蒸馏transformer模型，Park强制教师和学生中token和层之间的关系是一致的；Jiao把token之间的注意力依赖作为知识传给学生模型

与之前只考虑单一粒度表示方法不同，本论文中提到的方法同时转移token级、span级和文本级结构知识。

#### 3. 方法

多粒度结构知识蒸馏——MGSKD，从大型transformer语言模型中提取到的一个小型模型。知识来自于三个粒度：token级、span级和文本级，把不同粒度表示的pair-wise interactions和triplet-wise geometric angles等结构关系信息作为知识进行传递，并分层的将结构知识传递给学生模型；token级和span级给学生模型的底层提供句法指导，sample级知识给学生模型的上层来帮助语义理解。

![2b86ab6bac08566dac7d58245280898b_2_Figure_1](D:\d-desk\选课\轻量级人工智能\ACL 2022\4.20汇报论文\MGSKD\2b86ab6bac08566dac7d58245280898b_2_Figure_1.png)

##### 3.1 多粒度表示

以前的知识蒸馏大都只考虑单粒度的知识的表示，作者认为，通过将不同粒度的表示方式融合到知识蒸馏中，可以帮助学生模型学习如何从小粒度的语义概念组合成更大的语义概念。这样做可以提供更全面的指导，从而提高学生模型的性能。

* **Token Representation** 

  对于给定的输入文本，首先通过一个分词器（WordPiece）被划分成n个token，接着通过一个Embedding层形成一系列连续的表示$H^{0} = E$，最后通过L层堆叠的transformer层形成最终的token表示$H^{l}$

* **Span Representation**

  把由多个子词组成的单词和一些短语作为span，单词可以直接通过WordPiece直接获得；在CoNLL-2000语料库上训练了一个分块器来提取名词短语、动词短语和介词短语，通过WordPiece分词器来获得标记$x_{span} = [s_1, s_2, ...,s_{n_{s}}]$ $s_{i} = [t_j, t_{j+1},...,t_{j+n_{s_{i}}-1}]$ 并将生成的token表示通过平均池化获得最后的span表示

  ![image-20230417095900318](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230417095900318.png)

* **Sample Representation**

  使用平均池化聚合所有的token表示   

  ![image-20230417095930832](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230417095930832.png)

##### 3.2 结构知识提取

论文中把知识指定为表示的结构关系而不仅仅是表示本身，提取多粒度表示的结构关系作为知识传递给学生模型，将表示投影到多个子空间随后提取两种类型的结构知识：pairwise interactions and triplet-wise geometric angles

* **Multi-head Modeling**

  设置多个关系头提取知识已经被证明能够帮助学生模型得到更好的效果，因此在提取结构知识前先将表示映射到m个子空间中（线性投影到 m 个维度为 d/m 的子空间）——多头建模

* **Pair-wise Interaction**

  每对表示的多头成对交互特征为$P \in \mathbb{R}^{m \times n \times n}$ $P_{h,i,j}$ 代表第$h$个关系头的子空间中第$i$和第j和第$j$个表示的关系，$P$可以被认为是非归一化的自注意力分数，不同的在于$q = k$ 都多粒度的表示

  ![image-20230417103805323](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230417103805323.png)

* **Triplet-wise Geometric Angle**

  Pair-wise Interaction只能一次考虑两个向量的关系，不足以表示高维空间中表示之间的复杂结构关系，因此本论文中通过向量三元组计算几何夹角得到向量之间的关系作为结构知识

  "geometric angles" 指的是一组向量之间的夹角。具体来说，对于给定的向量集合 $X={x_1, x_2, ..., x_n}$，可以计算任意三个向量 $x_i, x_j, x_k$ 之间的夹角，并将这些夹角作为一部分知识来辅助知识蒸馏过程

  ![image-20230418182534913](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418182534913.png)

  为所有的三元组计算几何夹角结果为$T \in \mathbb{R}^{m \times n \times n \times n} $   $T_{h,i, j, k}$ 表示第h个关系头的子空间中向量$r_i, r_j, r_k$ 的几何夹角，计算复杂度是$O(n^3)$ 

  为了解决计算过于复杂的问题，论文提出了一个两阶段选择策略来选择较为重要的表示子集，在此子集中进行计算——从其他表示收到的attention越多，这个表示就越重要

  * 在$P$ 的最后一层添加$softmax$层来计算自注意力分布$A \in \mathbb{R}^{m \times n \times n}$

  * 对于第 $j$ 个表示，计算**全局显著分数**$s_j$，以此为基础选择前$k_1$个表示作为顶点

    **全局显著得分**被定义为一个语言单元（如单词或子词）与所有其他语言单元之间的注意力权重的总和。

    ![image-20230418213255612](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418213255612.png)

  * 对于第$i$个被选作为顶点的表示，以**局部显著分数**$s_{i, j}$ 为基础，选择$k_2$个表示来与此顶点作几何夹角运算

    **局部显著得分**是通过在固定的窗口大小内计算语言单元与其他语言单元之间的相似度得分来计算的。

    ![image-20230418213228724](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418213228724.png)

  计算复杂度从$O(mn^3)$ 减少到$O(mk_1k_2^2)$ ，通过控制$k_1$和$k_2$，可以控制任意数量的三元组几何夹角的运算

##### 3.3 分层蒸馏

通过3.2节提到的结构知识提取方法提取来自三个粒度的表示的知识，基于底层捕获语法特征，上层编码语义特征，为学生模型提出分层蒸馏——把不同粒度的知识传递给不同的层

1. 定义一个层映射$g(·)$ 将每一个学生层映射到它学习的教师层，对于$g(·)$采用**统一策略**

2. 将token级和span级知识传递给学生模型的前$M$层，将sample级知识传递给后$L_s + 1 -M$层

* **Token- and Span-level**

  * 对于给定的token级表示和span级表示${\{H_t^l, \hat H_t^l\}}_{l=0}^{L_t}$ 根据公式计算表示之间的成对交互和几何角度，${\{P_t^l, \hat P_t^l\}}_{l=0}^{L_t}$  和 ${\{T_t^l, \hat T_t^l\}}_{l=0}^{L_t}$ 

  * 同理，计算学生模型的结构关系：${\{P_s^l, \hat P_s^l\}}_{l=0}^{L_s}$  和 ${\{T_s^l, \hat T_s^l\}}_{l=0}^{L_s}$ 

  * 最小化教师和学生之间表示的结构关系的差异来进行知识传递

    ![image-20230418224353680](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418224353680.png)

* **Sample-level**

  论文中提到在小批量中收集所有的样本表示，来计算它们之间的结构关系作为样本级知识且只关注三元关系$\{\widetilde{T}_t^l\}_{l=0}^{L_t}$ 和 $\{\widetilde{T}_s^l\}_{l=0}^{L_s}$

  ![image-20230418225522781](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418225522781.png)

$l_1$ 和 $l_2$是损失函数，测量教师与学生之间的结构关系的距离，一般$l_1$是MSE，$l_2$是Huber loss

Huber loss是一种平滑的损失函数，通常用于回归问题中。它是均方误差（MSE）和绝对误差（MAE）之间的一种折中，旨在减少MSE对离群值的敏感性，同时保留MAE对于数据中所有点的影响。

Huber loss的公式如下：

$L_\delta(y_i,f(x_i)) = \begin{cases} \frac{1}{2}(y_i-f(x_i))^2 & \text{if } |y_i - f(x_i)| \leq \delta, \ \delta(|y_i - f(x_i)| - \frac{1}{2}\delta) & \text{otherwise,} \end{cases}$

其中，$y_i$是第i个样本的真实标签，$f(x_i)$是模型的预测值，\delta是一个超参数，用于控制Huber loss的平滑程度。当$|y_i - f(x_i)|$小于或等于$\delta$时，使用MSE计算损失；否则，使用MAE计算损失，并且对于误差大于$\delta$的点，损失的斜率在$\delta$处截断，使得误差对损失的贡献减少。

Huber loss可以减少MSE对于离群值的敏感性，同时保留MAE对于数据中所有点的影响。它常用于训练回归模型，特别是在存在离群值的情况下。

* **Overall Objectives**

  * 多粒度结构知识蒸馏的总体目标：

    ![image-20230418230631766](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418230631766.png)

    $\lambda_1$、$\lambda_2$ 和 $\lambda_3$ 分别为不同粒度损失函数的权重

  * 学生模型与教师模型的预测分布的差异：

    ![image-20230418230752419](C:\Users\ccwangxin\AppData\Roaming\Typora\typora-user-images\image-20230418230752419.png)

    $z_t和z_s$分别为教师模型和学生模型的预测概率分布，$\tau$为温度因子

#### 4 实验

##### 4.1数据集和评价指标

* GLUE数据集：
  * 两个单句任务：SST-2、CoLA
  * 三个相似性和释义任务：MRPC、STS-B、QQP
  * 四个推断任务：MNLI、QNLI、RTE、WNLI（未用）
* 评价指标：
  * 准确率：SST-2, QQP, MNLI, QNLI、RTE
  * F1 score：MRPC
  * Matthews相关系数：CoLA 
  * Spearman的等级相关系数：STS-B

##### 4.2 实施细节

* 专注于特定任务，进行数据增强
* 在原始训练集上微调$ELECTRA_{base}$ 作为教师模型，$TinyBERT-4-312^4$ 作为学生模型的初始化
* 对于token即和span级蒸馏，为二元交互设置64个关系头，为三元夹角设置1个head（处于计算和内存成本考虑）且计算三元夹角时设置$k_1 = k_2 = 20$
* 对于sample级蒸馏，使用64个关系头并且设置$k_1 = k_2 = batch \ size$
* 学生模型的前两层学习token级和span级知识，其余层学习sample级知识
* 对于结构蒸馏目标设置$\lambda_1 = 4$，$\lambda_2 = \lambda_3 = 1$
* 以多粒度结构知识蒸馏为目标在CoLA数据集上训练50个epoch，在其他数据集上训练20个epoch，学习率设置为$1e-5$ 、batch szie为32
* 以学生模型与教师模型的预测分布的差异，在QQP和CoLA原始数据集上训练学生模型10个epoch，在其他数据集上使用增强的数据集训练学生模型3个epoch，$\tau = 1.0$ 学习率设置为$1e-5$ 、batch szie为32 

##### 4.3 对比方法

* **中型学生模型**
* **小型学生模型**

##### 4.4 主要结果

![2b86ab6bac08566dac7d58245280898b_5_Table_1](D:\d-desk\选课\轻量级人工智能\ACL 2022\4.20汇报论文\MGSKD\2b86ab6bac08566dac7d58245280898b_5_Table_1.png)

* 在相同的蒸馏设置下，MGSKD模型在七个任务上的表现都优于强基线模型
* 与具有更多的参数但在不同蒸馏设置条件的中等大小模型相比，MFSKD模型在大多数任务上表现也最优
* 通过更强的教师模型和数据增强技术，MGSKD使学生模型在大多数任务上实现与$BERT_{base}$ 相同的性能，同时保持9.4倍的加速率
* MGSKD 在大多数 GLUE 任务上表现良好，但它落后于 CoLA 上的一些基线，原因可能是CoLA需要模型关注语义信息而不是句子水平的语法信息，减少了论文中提到的多粒度语义信息的需要

##### 4.5 讨论

* 关系头的影响

  下表是关系头的数量对于SST-2和MNLI数据集性能影响，可以看出随着m的增加，模型在两个数据集上的表现也是越来越好的，通过在多个相对低维空间中提供细粒度监督，简化了学生在非常高维向量空间中学习结构关系的困难，随着m的增加所需的计算量和内存占用也在增加，因此当m=128时性能略微下降，因此选择m=64

  ![2b86ab6bac08566dac7d58245280898b_6_Table_2](D:\d-desk\选课\轻量级人工智能\ACL 2022\4.20汇报论文\MGSKD\2b86ab6bac08566dac7d58245280898b_6_Table_2.png)

* 知识粒度的消融实验

  论文中提到把三个粒度的结构知识转移到学生模型，并在token级和span级提取二元关系，在sample级提取三元关系；为了证明这种处理方式的有效性进行了消融实验

  1. 分别从MGSKD的目标中删除每个粒度的知识，发现sample级知识最重要，其次是token级，最后是span级，但每个粒度的知识都能对模型产生积极的影响
  2. 对于每个粒度研究每种形式的结构知识对模型性能的影响，为了进行公平比较，将每个粒度的知识提取到所有的学生层。实验发现，对于token级和span级二元关系比三元关系更有效，联合两者的模型表现更好；而对于sample级三元关系远远优于二元关系，而联合两者也不能使模型得到更好的表现，因此实验中只使用三元组关系作为样本级知识

  ![2b86ab6bac08566dac7d58245280898b_6_Table_3](D:\d-desk\选课\轻量级人工智能\ACL 2022\4.20汇报论文\MGSKD\2b86ab6bac08566dac7d58245280898b_6_Table_3.png)

* $k_1$和$k_2$对于计算角度的影响

  为了减少计算复杂度，设置了两个超参数$k_1$和$k_2$选择重要的表示来计算角度，并分别测试了token级和sample级三元关系中两个参数的选择对于模型性能的影响，并设置$k_1 = k_2$

  * 对于token级，当参数较小时，随着参数的增大模型的精确度越高；而大于20时候，精确度有所下降，因此对于token级角度计算选择$k_1 = k_2 = 20$
  * 对于sample级特征的三元关系，模型精确度随着参数的增大单调增加，因此对于sample级角度计算选择$k_1 = k_2 = batch \ size$

  ![2b86ab6bac08566dac7d58245280898b_7_Figure_3](D:\d-desk\选课\轻量级人工智能\ACL 2022\4.20汇报论文\MGSKD\2b86ab6bac08566dac7d58245280898b_7_Figure_3.png)

* 边界层M的选择

  论文中提到了分层蒸馏策略，将token级和span级知识传递给学生底部M层，将样本级知识传递到其余上层，为了验证有效性并寻找最好的M，图中展示了不同的M对于模型准确度的影响

  * 虚线表示将所有粒度的知识提取到学生模型所有的层
  * 实线表示分层蒸馏设置
    * M=0时，学生只学习sample级知识，M=4学生学习token级和span级知识
    * 在没有其他知识粒度的帮助下，学生在这两个任务上的表现相对较差
    * 随着M的增加，模型的性能曲线超过了虚线，证明了分层蒸馏策略的有效性
    * 当M=2是模型精确度最高

#### 5 总结

论文中的MGSKD知识蒸馏框架，利用多粒度语言单元的中间表示，并表示之间复杂的结构关系作为知识传递给学生模型，并采用分层蒸馏策略进行知识学习，在GLUE基准测试集上验证了其方法的有效性

